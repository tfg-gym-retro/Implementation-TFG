{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default CartPole with Q-Learning\n",
    "\n",
    "## *TFG Reinforcement Learning through the GymRetro Platform.*\n",
    "\n",
    "In this notebook we will show how to load and train a Tensorforce DQN agent in the Gym CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJSR8OszdqY"
   },
   "source": [
    "## Previous installs:\n",
    "\n",
    "First we will install __Gym__, a library by _OpenAI_ that provides different environments for reinforcement learning.\n",
    "\n",
    "We will also install __Tensorforce__ which provides an easy way to create Deep Reinforcement Learning agents that interact with these environments, and other required installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23750,
     "status": "ok",
     "timestamp": 1649714508609,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "L4AqbHXbzdqa",
    "outputId": "8dced317-1794-425a-c9ce-18c4278f8582"
   },
   "outputs": [],
   "source": [
    "!pip install gym[all]==0.21.0\n",
    "!pip install tensorforce\n",
    "!pip install keras==2.6.0\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1649714508609,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "TIicJYxLzdqa"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorforce import Agent, Environment\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdG0iOcKzdqc"
   },
   "source": [
    "## Creation or loading of the agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYjNRto4zdqc"
   },
   "source": [
    "Now we will create a _Deep Q-Learning_ Tensorforce agent that should learn to move the cart in a way that keeps the pole from tilting. Tensorforce has integrated support for gym environments, which will make the implementation much easier.\n",
    "\n",
    "The information that the environment provides the agent has the following format:\n",
    "[position of cart, velocity of cart, angle of pole, angular velocity of pole].\n",
    "\n",
    "Execute the first cell if it's your first time training the agent, or execute the second cell if you want to load an existing agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "error",
     "timestamp": 1649714509471,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "z62xujU_zdqc",
    "outputId": "f554a4fc-5003-41db-8d35-c6773c2b97e5"
   },
   "outputs": [],
   "source": [
    "# Add parameter visualize='True' if we want to see the training process. Slower.\n",
    "environment = Environment.create(environment='gym', level='CartPole-v1')\n",
    "\n",
    "# Instantiate a Tensorforce agent\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
    "    memory=50000,\n",
    "    batch_size=32,\n",
    "    # Save agent every 100 updates and keep the 5 most recent checkpoints\n",
    "    saver=dict(directory='Agent_directory', frequency=100, max_checkpoints=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load(directory='Agent_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(environment='gym', level='CartPole-v1')\n",
    "\n",
    "episode_reward = []\n",
    "episodeTimes = []\n",
    "episodeTimeSteps = []\n",
    "\n",
    "trainingStart = time.time()\n",
    "\n",
    "# Train for 10000 episodes\n",
    "for episode in range(10000):\n",
    "\n",
    "    # Initialize episode\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    rewardTotal = 0\n",
    "    currentEpisodeTimeSteps = 0\n",
    "    episodeStart = time.time()\n",
    "    while not terminal:\n",
    "        # Episode timestep       \n",
    "        currentEpisodeTimeSteps += 1\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "        rewardTotal += reward\n",
    "     \n",
    "    episodeEnd = time.time()\n",
    "    timeEpisode = episodeEnd - episodeStart\n",
    "    episodeTimes.append(timeEpisode)\n",
    "    episode_reward.append(rewardTotal)\n",
    "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episode: {episode}\")\n",
    "    \n",
    "    \n",
    "trainingEnd = time.time()\n",
    "trainingTime = trainingEnd - trainingStart\n",
    "environment.close()\n",
    "\n",
    "print(f\"Elapsed training time: {trainingTime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some data gathered during the training into files so we can plot it and evaluate the evolution of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards_per_episode.txt', 'w') as f:\n",
    "    for item in episode_reward:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('timesteps_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimeSteps:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('times_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of out trained agent:\n",
    "\n",
    "We check the perfomance of an already trained agent without training it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_DQVM8Bzdqe",
    "outputId": "ee61fb4b-a70d-43b6-c04f-cb46d8cc9f42"
   },
   "outputs": [],
   "source": [
    "agent = Agent.load(directory='DQNCartPolemodel4')\n",
    "environment = Environment.create(environment='gym', level='CartPole', max_episode_timesteps=10000)\n",
    "# Uncomment the next line if want to see what the agent is doing.\n",
    "environment.visualize = 'True'\n",
    "\n",
    "episodeTimes = []\n",
    "episodeTimeSteps = []\n",
    "for _ in range(10):\n",
    "    episodeStart = time.time()\n",
    "    # Initialize episode\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    currentEpisodeTimeSteps = 0\n",
    "    while not terminal:\n",
    "        # Episode timestep\n",
    "        currentEpisodeTimeSteps += 1\n",
    "        actions = agent.act(states=states, independent = True, deterministic=True)\n",
    "        #print(actions)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "    \n",
    "    episodeEnd = time.time()\n",
    "    timeEpisode = episodeEnd - episodeStart\n",
    "    episodeTimes.append(timeEpisode)\n",
    "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
    "    \n",
    "environment.close()\n",
    "    \n",
    "avgEpisodeTime = sum(episodeTimes) / len(episodeTimes)\n",
    "bestEpisodeTime = max(episodeTimes)\n",
    "avgEpisodeTimeSteps = sum(episodeTimeSteps) / len(episodeTimeSteps)\n",
    "bestEpisodeTimeSteps = max(episodeTimeSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sInRao7ezdqe"
   },
   "source": [
    "## Check results of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzQKSspxzdqe",
    "outputId": "cc0a8da7-90fe-49fc-be93-771e5c2201d6"
   },
   "outputs": [],
   "source": [
    "print(f\"Average time steps per episode: {avgEpisodeTimeSteps} timesteps\")\n",
    "print(f\"Best episode: {bestEpisodeTimeSteps} timesteps\")\n",
    "print(f\"Average episode duration: {avgEpisodeTime} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CartPole TensorForce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
