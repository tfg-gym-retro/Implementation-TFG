{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default CartPole with Q-Learning\n",
    "\n",
    "## *TFG Reinforcement Learning through the GymRetro Platform.*\n",
    "\n",
    "In this notebook we will show how to implement the Q-Learning algorithm for the CartPole environment integrated in Gym, in order to train agents in that environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous installs:\n",
    "\n",
    "Only run this cell if you don't have Gym installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import time\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classic approach: Q-Learning\n",
    "\n",
    "The main obstacle to implement the Q-Learning algorithm in this environment will be to discretize the state space, which, in the CartPole problem is continuous by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we explore the environment attributes given by gym, as we observe, the action space is made up of two discreet actions (which correspond with applying a force of +1 or -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\").env\n",
    "print(env.action_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is far more complex: an object of type Box which representes a 4-dimensional continuous space, and each state is a tuple (Cart position, Cart Velocity, Pole Angle, Pole Velocity) which is a symbolic representation of the absolute state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peeking into the observation space to uncover and understand its structure\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a brief inspection of the environment, we start to implement the q-learning algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning: \n",
    "\n",
    "Reward table (from the environment): P-table\n",
    "\n",
    "Q-table: stores the _Q-value_ (the 'quality' of an action) associated to a `(state,action)` combination\n",
    "\n",
    "\n",
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
    "\n",
    "Q(state,action) ← (1−α) Q(state,action) + α(reward + γ maxa Q(next state,all actions))\n",
    "\n",
    "Where:\n",
    "- α (alpha) is the learning rate (0<α≤1)\n",
    "- γ (gamma) is the discount factor (0≤γ≤1)\n",
    "\n",
    "Q-Table values are initialized to a random value and then updated during training to values that optimize the agent's traversal through the\n",
    "environment for maximum rewards\n",
    "\n",
    "Steps:\n",
    "- Initialize the Q-table.\n",
    "- Start exploring actions: For each state, select any one among all possible actions for the current state (S). - Travel to the next state (S') as a result of that action (a).\n",
    "- For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "- Update Q-table values using the equation.\n",
    "- Set the next state as the current state.\n",
    "- If goal state is reached, then end and repeat the process.\n",
    "\n",
    "We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called ε \"epsilon\" to cater to this during training.\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learning parameters\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 1\n",
    "epsilon_decay_value = 0.99995\n",
    "\n",
    "#Training-specific values\n",
    "EPISODES = 500000\n",
    "total_time = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "#Discretization values\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the dimensions of the Q-Table and an example of its initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialization of the q-table: for each state and action a random value is asigned\n",
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "print(q_table.shape)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the discretization of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretization function\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_reward = []\n",
    "episodeTimes = []\n",
    "episodeTimeSteps = []\n",
    "\n",
    "episodeTrainingTimes = []\n",
    "trainingStart = time.time()\n",
    "for episode in range(EPISODES): #go through the episodes\n",
    "    episodeTrainingStart = time.time()\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 #reward starts as 0 for each episode\n",
    "    \n",
    "    currentEpisodeTimeSteps = 0\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) #do a random ation\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward #add the reward\n",
    "        \n",
    "        currentEpisodeTimeSteps += 1\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: #render\n",
    "            env.render()\n",
    "\n",
    "        if not done: #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: #epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() #episode has finished\n",
    "    episode_total = t1 - t0 #episode total time\n",
    "    episodeTimes.append(episode_total)\n",
    "    episodes_reward.append(episode_reward)\n",
    "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
    "    total_time = total_time + episode_total\n",
    "\n",
    "    total_reward += episode_reward #episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
    "        mean = total_time / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "    \n",
    "trainingEnd = time.time()\n",
    "trainingTime = trainingEnd - trainingStart\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some of the training data into files so later on we can plot them and check the evolution of the agent through the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards_per_episode.txt', 'w') as f:\n",
    "    for item in episodes_reward:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('timesteps_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimeSteps:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('times_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of our trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "total_epochs, total_rewards = 0, 0\n",
    "episodes = 1\n",
    "episodeTimes = []\n",
    "episodeTimesteps = []\n",
    "for _ in range(episodes):\n",
    "    episodeStart = time.time()\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    currentTimesteps = 0\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[get_discrete_state(state)])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        currentTimesteps += 1\n",
    "    \n",
    "    total_rewards += rewards\n",
    "    episodeEnd = time.time()\n",
    "    timeEpisode = episodeEnd - episodeStart\n",
    "    episodeTimes.append(timeEpisode)\n",
    "    episodeTimesteps.append(currentTimesteps)\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "#Some metrics\n",
    "avgEpisodeTime = sum(episodeTimes) / len(episodeTimes)\n",
    "bestEpisodeTime = max(episodeTimes)\n",
    "avgEpisodeTimesteps = sum(episodeTimesteps) / len(episodeTimes)\n",
    "bestEpisodeTimesteps = max(episodeTimesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results of training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {avgEpisodeTimesteps} timesteps\")\n",
    "print(f\"Best episode: {bestEpisodeTimesteps} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration of episodes results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training time: {trainingTime} seconds\")\n",
    "print(f\"Average seconds per episode after training: {avgEpisodeTime} seconds\")\n",
    "print(f\"Longest episode: {bestEpisodeTime} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
