{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NES Space Invaders with RAM**\n",
        "## *TFG Reinforcement Learning through the GymRetro Platform.*\n",
        "\n",
        "In this notebook we will show how to train and load a Tensorforce DQN agent that plays NES' Space Invaders using the Arcade Learning Environment integrated in Gym and GymRetro."
      ],
      "metadata": {
        "id": "Bsrp9YoschRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous installs:\n",
        "Run the following cell only if you are using Google Colab, or if you still haven't installed locally the required libraries."
      ],
      "metadata": {
        "id": "RwzbS7LbcqWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwM0SN6JLdkD"
      },
      "outputs": [],
      "source": [
        "!pip install gym-retro\n",
        "!pip install tensorforce\n",
        "# The following versions are required to work properly with the latest (0.6.5) version of Tensorforce.\n",
        "!pip install keras==2.6.0\n",
        "!pip install gym==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive:\n",
        "The following code allows the interaction between Google Colab and Google Drive. It will be useful for saving the trained agent."
      ],
      "metadata": {
        "id": "jTft5Nw-c2cL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxlJ2uMGEeOr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = './gdrive/My Drive/dir'  #Change 'dir' to the folder in which you want to store the result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Needed imports:"
      ],
      "metadata": {
        "id": "uJXXPbMOdHem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjR3DZKHLjoS"
      },
      "outputs": [],
      "source": [
        "import retro\n",
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorforce import Agent, Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptZ2x2_yDPYk"
      },
      "outputs": [],
      "source": [
        "# This code imports the required ROM into the system (SpaceInvaders-Nes).\n",
        "# Locally, the ROM may need to be imported diferently.\n",
        "!python3 -m retro.import './gdrive/My Drive/Path_to_your_rom'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discretizer\n",
        "\n",
        "The following code, adapted from [this](https://github.com/openai/retro/blob/master/retro/examples/discretizer.py), allows us to discretize all possible actions into just the few that interest us. The combo list may be changed as wanted."
      ],
      "metadata": {
        "id": "ZYvN5Hp3eIU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUOHYSIjmiXE"
      },
      "outputs": [],
      "source": [
        "class Discretizer(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Wrap a gym environment and make it use discrete actions.\n",
        "    Args:\n",
        "        combos: ordered list of lists of valid button combinations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, combos):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
        "        buttons = env.unwrapped.buttons\n",
        "        self._decode_discrete_action = []\n",
        "        for combo in combos:\n",
        "            arr = np.array([False] * env.action_space.n)\n",
        "            for button in combo:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._decode_discrete_action.append(arr)\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self._decode_discrete_action[act].copy()\n",
        "\n",
        "\n",
        "class SpaceInvadersNesDiscretizer(Discretizer):\n",
        "    def __init__(self, env):\n",
        "      # We allow the character to stay still, move either way, shoot standing still, or shoot while moving either way.\n",
        "      super().__init__(env=env, combos=[[], ['LEFT'], ['RIGHT'], ['A'], ['LEFT','A'], ['RIGHT','A']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creation or loading of agent:\n",
        "\n",
        "Execute the first cell if it's your first time creating the agent.\n",
        "Execute the second cell if you want to load your previously created agent.\n",
        "Throughout the notebook, change *AGENT_NAME* to the name of the agent you are loading or saving."
      ],
      "metadata": {
        "id": "NQpTn2BmeHf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = retro.make(game='SpaceInvaders-Nes', obs_type=retro.Observations.RAM)\n",
        "env = SpaceInvadersNesDiscretizer(env)\n",
        "environment = Environment.create(environment=env)\n",
        "\n",
        "# Instantiate a Tensorforce agent\n",
        "agent = Agent.create(\n",
        "    agent='dqn',\n",
        "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
        "    batch_size=32,\n",
        "    memory=1000,\n",
        "    exploration=0.05,\n",
        "    # Setting this to GPU will not work in Google Colab. Locally, it depends on having the right versions installed.\n",
        "    # See https://www.tensorflow.org/install/gpu?hl=es-419 for more on this\n",
        "    config=dict(device='CPU'),\n",
        "    # The following setting saves Tensorboard information in the choosen folder. It can be uncommented to enable it.\n",
        "    # summarizer=dict(\n",
        "    #    directory=root_path + 'NES/data/summaries/AGENT-NAME',\n",
        "    #    summaries='all'\n",
        "    # ),\n",
        "    # The following setting is necesary for applying posterior XAI techniques.\n",
        "    tracking = 'all'\n",
        ")"
      ],
      "metadata": {
        "id": "2YWuHmKWeyUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent.load(directory=root_path +'Atari/AGENT_NAME')"
      ],
      "metadata": {
        "id": "z7XypynXfbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent training:\n",
        "\n",
        "Load this cell if you want to start training from the beginning. Be careful not to execute it otherwise or you may lose previous data."
      ],
      "metadata": {
        "id": "t1NxLdpmfeI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cW8aUn-LdkH"
      },
      "outputs": [],
      "source": [
        "env = retro.make(game='SpaceInvaders-Nes', obs_type=retro.Observations.RAM)\n",
        "env = SpaceInvadersNesDiscretizer(env)\n",
        "environment = Environment.create(environment=env)\n",
        "\n",
        "episode_reward = []\n",
        "episodeTimes = []\n",
        "episodeTimeSteps = []\n",
        "trainingStart = time.time()\n",
        "\n",
        "# Train for 100 episodes\n",
        "for i in range(100):\n",
        "\n",
        "    # Initialize episode\n",
        "    states = environment.reset()\n",
        "    terminal = False\n",
        "    rewardTotal = 0\n",
        "    currentEpisodeTimeSteps = 0\n",
        "    episodeStart = time.time()\n",
        "\n",
        "    # Main training loop\n",
        "    while not terminal:\n",
        "        # Episode timestep\n",
        "        currentEpisodeTimeSteps += 1\n",
        "        actions = agent.act(states=states)\n",
        "        states, terminal, reward = environment.execute(actions=actions)\n",
        "        agent.observe(terminal=terminal, reward=reward)\n",
        "        rewardTotal += reward\n",
        "\n",
        "    # End of episode\n",
        "    episodeEnd = time.time()\n",
        "    timeEpisode = episodeEnd - episodeStart\n",
        "    episodeTimes.append(timeEpisode)\n",
        "    episode_reward.append(rewardTotal)\n",
        "    episodeTimeSteps.append(currentEpisodeTimeSteps)  \n",
        "    print('End of episode', i)\n",
        "    # Save episodes every 10 episodes\n",
        "    if len(episodeTimes) == 10:\n",
        "        with open(root_path +'NES/Episodes/AGENT_NAME/rewards_per_episode.txt', 'a') as f:\n",
        "            for item in episode_reward:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        \n",
        "        with open(root_path + 'NES/Episodes/AGENT_NAME/timesteps_per_episode.txt', 'a') as f:\n",
        "            for item in episodeTimeSteps:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        \n",
        "        with open(root_path +'NES/Episodes/AGENT_NAME/times_per_episode.txt', 'a') as f:\n",
        "            for item in episodeTimes:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        episode_reward = []\n",
        "        episodeTimes = []\n",
        "        episodeTimeSteps = []\n",
        "    # Save agent every 10 episodes\n",
        "    # This can potentially be done with the \"saver\" setting in the agent too.\n",
        "    if i % 10 == 9:\n",
        "      agent.save(directory=root_path + 'GymRetro/AGENT_NAME')\n",
        "\n",
        "agent.close()\n",
        "environment.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbmr4iMoK3DH"
      },
      "source": [
        "### Check results of training:\n",
        "\n",
        "You can run this cell to load your agent and check some metrics about the current performance.\n",
        "Our personal experience was that if we ran this code without agent.observe() and with agent.act() having the parameters independent and deterministic, the agent would get stuck performing a certain action over and over. This doesn't happen while actually training the agent, that is why here the agent is trained too. Feel free to try whichever approach works for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZCsyHbA5KC0"
      },
      "outputs": [],
      "source": [
        "agent = Agent.load(directory=root_path +'NES/AGENT_NAME')\n",
        "env = retro.make(game='SpaceInvaders-Nes', obs_type=retro.Observations.RAM, record = root_path)\n",
        "env = SpaceInvadersNesDiscretizer(env)\n",
        "environment = Environment.create(environment=env)\n",
        "\n",
        "episodeTimes = []\n",
        "episodeTimeSteps = []\n",
        "episodeRewards = []\n",
        "for _ in range(2):\n",
        "    episodeStart = time.time()\n",
        "    # Initialize episode\n",
        "    states = environment.reset()\n",
        "    terminal = False\n",
        "    currentEpisodeTimeSteps = 0\n",
        "    currentReward = 0\n",
        "    while not terminal:\n",
        "        # Episode timestep\n",
        "        currentEpisodeTimeSteps += 1\n",
        "        actions = agent.act(states=states)\n",
        "        # May want to try adding 'independent = True' and 'deterministic = True', as well as removing agent.observe\n",
        "        states, terminal, reward = environment.execute(actions=actions)\n",
        "        agent.observe(terminal=terminal, reward=reward)\n",
        "        currentReward += reward\n",
        "    \n",
        "    episodeEnd = time.time()\n",
        "    timeEpisode = episodeEnd - episodeStart\n",
        "    episodeTimes.append(timeEpisode)\n",
        "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
        "    episodeRewards.append(currentReward)\n",
        "    \n",
        "environment.close()\n",
        "    \n",
        "avgEpisodeTime = sum(episodeTimes) / len(episodeTimes)\n",
        "bestEpisodeTime = max(episodeTimes)\n",
        "avgEpisodeTimeSteps = sum(episodeTimeSteps) / len(episodeTimeSteps)\n",
        "bestEpisodeTimeSteps = max(episodeTimeSteps)\n",
        "avgEpisodeReward = sum(episodeRewards) / len(episodeRewards)\n",
        "bestEpisodeReward = max(episodeRewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7Wy1nGz6D_B"
      },
      "outputs": [],
      "source": [
        "print(f\"Average time steps per episode: {avgEpisodeTimeSteps} timesteps\")\n",
        "print(f\"Episode with most timesteps: {bestEpisodeTimeSteps} timesteps\")\n",
        "print(f\"Average reward per episode: {avgEpisodeReward}\")\n",
        "print(f\"Best episode reward: {bestEpisodeReward}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "GymRetro RAM acciones discretizadas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}