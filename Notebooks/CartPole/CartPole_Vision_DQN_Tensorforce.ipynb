{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default CartPole with Q-Learning\n",
    "\n",
    "## *TFG Reinforcement Learning through the GymRetro Platform.*\n",
    "\n",
    "In this notebook we will show how to load and train a Tensorforce DQN agent in the Gym CartPole environment, taking the screen as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649700519125,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "eJHgdzyWzclX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the original work of: `Adam Paszke <https://github.com/apaszke>`_\n",
    "[**Original code can be found here**](https://github.com/pytorch/tutorials/blob/master/intermediate_source/reinforcement_q_learning.py)\n",
    "\n",
    "License for the original code:\n",
    "\n",
    "BSD 3-Clause License\n",
    "\n",
    "Copyright (c) 2017-2022, Pytorch contributors\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "* Neither the name of the copyright holder nor the names of its\n",
    "  contributors may be used to endorse or promote products derived from\n",
    "  this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JWe7TBUzclY"
   },
   "source": [
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "However, neural networks can solve the task purely by looking at the\n",
    "scene, so we'll use a patch of the screen centered on the cart as an\n",
    "input. Because of this, our results aren't directly comparable to the\n",
    "ones from the official leaderboard - our task is much harder.\n",
    "Unfortunately this does slow down the training, because we have to\n",
    "render all the frames.\n",
    "\n",
    "Strictly speaking, we will present the state as the difference between\n",
    "the current screen patch and the previous one. This will allow the agent\n",
    "to take the velocity of the pole into account from one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23403,
     "status": "ok",
     "timestamp": 1649701573822,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "cHohOUOzzclb",
    "outputId": "da61e836-5572-4c2b-eccd-cecd9072a361"
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tensorforce\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1649701577823,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "QpEMnxbwzclb"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorforce import Agent, Environment\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment:\n",
    "\n",
    "We manually set our environment, adding functions to get the screen of each timestep, as well as apply some changes to it so it's easier to process every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1649701580665,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "4sqwKDo0zcld",
    "outputId": "9d6fff88-b7c9-4589-c66d-d259350690bd"
   },
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class CartPoleVisionEnvironment(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.init_screen = self.get_screen()\n",
    "        self.init_state = (self.get_screen() - self.init_screen).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        self.last_screen = self.init_screen\n",
    "        self.current_screen = self.init_screen\n",
    "        super().__init__()\n",
    "        \n",
    "    def get_cart_location(self, screen_width):\n",
    "        world_width = self.env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(self.env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        \n",
    "    def get_screen(self):\n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def is_vectorizable(self):\n",
    "        return True                          \n",
    "    \n",
    "    def actions(self):\n",
    "        return dict(type='int', shape=(), num_values=2)\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=self.init_state.shape)\n",
    "\n",
    "    def execute(self, actions):\n",
    "        _, reward, done, _ = self.env.step(actions)\n",
    "        self.last_screen = self.current_screen\n",
    "        self.current_screen = self.get_screen()\n",
    "        if not done:\n",
    "            next_state = (self.current_screen - self.last_screen).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        else: next_state = None\n",
    "        return next_state, done, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.last_screen = self.init_screen\n",
    "        self.current_screen = self.init_screen\n",
    "        return self.init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation or loading of the agent:\n",
    "\n",
    "Execute the first cell if it's your first time training the agent, or execute the second cell if you want to load an existing agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment =  Environment.create(environment = CartPoleVisionEnvironment, max_episode_timesteps=10000)\n",
    "\n",
    "# Instantiate a Tensorforce agent\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
    "    memory=10000,\n",
    "    batch_size=32,\n",
    "    exploration=0.05,\n",
    "    # Save agent every 100 updates and keep the 5 most recent checkpoints\n",
    "    saver=dict(directory='Agent_directory', frequency=100, max_checkpoints=5),\n",
    "    tracking = 'all',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load(directory='Agent_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beicASK7zcle"
   },
   "source": [
    "## Agent training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "error",
     "timestamp": 1649701584690,
     "user": {
      "displayName": "MANUEL ORTEGA SALVADOR",
      "userId": "05345466687059021496"
     },
     "user_tz": -120
    },
    "id": "XVaH7NzTzcle",
    "outputId": "570f9286-5e5c-4ff4-b94a-89968640d564"
   },
   "outputs": [],
   "source": [
    "environment =  Environment.create(environment = CartPoleVisionEnvironment, max_episode_timesteps=10000)\n",
    "\n",
    "episode_reward = []\n",
    "episodeTimes = []\n",
    "episodeTimeSteps = []\n",
    "\n",
    "trainingStart = time.time()\n",
    "\n",
    "# Train for 10000 episodes\n",
    "for episode in range(10000):\n",
    "\n",
    "    # Initialize episode\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    rewardTotal = 0\n",
    "    currentEpisodeTimeSteps = 0\n",
    "    episodeStart = time.time()\n",
    "    while not terminal:\n",
    "        # Episode timestep\n",
    "        currentEpisodeTimeSteps += 1\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "        rewardTotal += reward\n",
    "    \n",
    "    episodeEnd = time.time()\n",
    "    timeEpisode = episodeEnd - episodeStart\n",
    "    episodeTimes.append(timeEpisode)\n",
    "    episode_reward.append(rewardTotal)\n",
    "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episode: {episode}\")\n",
    "    \n",
    "trainingEnd = time.time()\n",
    "trainingTime = trainingEnd - trainingStart\n",
    "environment.close()\n",
    "\n",
    "print(f\"Elapsed training time: {trainingTime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load some data gathered during the training into files so we can plot it and evaluate the evolution of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards_per_episode.txt', 'w') as f:\n",
    "    for item in episode_reward:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('timesteps_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimeSteps:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('times_per_episode.txt', 'w') as f:\n",
    "    for item in episodeTimes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of out trained agent:\n",
    "\n",
    "We check the perfomance of an already trained agent without training it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw26Y-BVzclg",
    "outputId": "8e10743e-5552-4393-b075-3a3aa275399d"
   },
   "outputs": [],
   "source": [
    "agent = Agent.load(directory='Agent_directory')\n",
    "environment =  Environment.create(environment = CartPoleVisionEnvironment, max_episode_timesteps=10000)\n",
    "\n",
    "episodeTimes = []\n",
    "episodeTimeSteps = []\n",
    "for _ in range(10):\n",
    "    episodeStart = time.time()\n",
    "    # Initialize episode\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    currentEpisodeTimeSteps = 0\n",
    "    while not terminal:\n",
    "        # Episode timestep\n",
    "        currentEpisodeTimeSteps += 1\n",
    "        actions = agent.act(states=states, independent = True, deterministic=True)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "    \n",
    "    episodeEnd = time.time()\n",
    "    timeEpisode = episodeEnd - episodeStart\n",
    "    episodeTimes.append(timeEpisode)\n",
    "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
    "    \n",
    "environment.close()\n",
    "    \n",
    "avgEpisodeTime = sum(episodeTimes) / len(episodeTimes)\n",
    "bestEpisodeTime = max(episodeTimes)\n",
    "avgEpisodeTimeSteps = sum(episodeTimeSteps) / len(episodeTimeSteps)\n",
    "bestEpisodeTimeSteps = max(episodeTimeSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NM9Q9JAzclh",
    "outputId": "464dfe22-4402-40c5-b42e-5976e71c2b8a"
   },
   "outputs": [],
   "source": [
    "print(f\"Average time steps per episode: {avgEpisodeTimeSteps} timesteps\")\n",
    "print(f\"Best episode: {bestEpisodeTimeSteps} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcrhNUQFzclh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Training time: {trainingTime} seconds\")\n",
    "print(f\"Average seconds per episode after training: {avgEpisodeTime} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cartpole Vision DQN Tensorforce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
