{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Atari 2600 Space Invaders with Vision**\n",
        "## *TFG Reinforcement Learning through the GymRetro Platform.*\n",
        "\n",
        "In this notebook we will show how to train and load a Tensorforce DQN agent that plays Atari 2600's Space Invaders using the Arcade Learning Environment integrated in Gym and GymRetro."
      ],
      "metadata": {
        "id": "LrJExvf308Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous installs:\n",
        "Run the following cell only if you are using Google Colab, or if you still haven't installed locally the required libraries."
      ],
      "metadata": {
        "id": "F7a8pjb62-Ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd19ShhM1BPB"
      },
      "outputs": [],
      "source": [
        "!pip install tensorforce\n",
        "# The following versions are required to work properly with the latest (0.6.5) version of Tensorforce.\n",
        "!pip install keras==2.6.0\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google drive:\n",
        "The following code allows the interaction between Google Colab and Google Drive. It will be useful for saving the trained agent."
      ],
      "metadata": {
        "id": "kc8EBNAX07Fz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jLnxGPA1Ecc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/dir'  #Change 'dir' to the folder in which you want to store the result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Needed imports:"
      ],
      "metadata": {
        "id": "hN62NK3l4GSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7chjuiA1Iau"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorforce import Agent, Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creation or loading of agent:\n",
        "\n",
        "Execute the first cell if it's your first time creating the agent.\n",
        "Execute the second cell if you want to load your previously created agent."
      ],
      "metadata": {
        "id": "9YSSwNWn282x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "environment = Environment.create(environment='gym', level='SpaceInvaders-v0')\n",
        "\n",
        "# Instantiate a Tensorforce agent\n",
        "agent = Agent.create(\n",
        "    agent='dqn',\n",
        "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
        "    batch_size=32,\n",
        "    memory=10000,\n",
        "    # Setting this to GPU will not work in Google Colab. Locally, it depends on having the right versions installed.\n",
        "    # See https://www.tensorflow.org/install/gpu?hl=es-419 for more on this\n",
        "    config=dict(device='CPU'),\n",
        "    # This setting allows the agent to preprocess the state. It can be edited to a diferent preprocessing, or deleted for no preprocessing.\n",
        "    state_preprocessing = [\n",
        "        dict(type='image', height=105, width=80, grayscale=True),\n",
        "        dict(type='linear_normalization')               \n",
        "    ],\n",
        "    # The following setting saves Tensorboard information in the choosen folder. It can be uncommented to enable it.\n",
        "    #summarizer=dict(\n",
        "    #    directory=root_path + 'Atari/data/summaries/AGENT_NAME',\n",
        "    #    summaries='all'\n",
        "    #),\n",
        "    # The following setting is necesary for applying posterior XAI techniques.\n",
        "    tracking = 'all'\n",
        ")"
      ],
      "metadata": {
        "id": "cjWA9zYx3Iky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent.load(directory=root_path +'Atari/AGENT_NAME')"
      ],
      "metadata": {
        "id": "BvbYOcqo3LPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_aAgeZP1K_C"
      },
      "source": [
        "### Agent training:\n",
        "\n",
        "Load this cell if you want to start training from the beginning. Be careful not to execute it otherwise or you may lose previous data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqCDUA-51NKj"
      },
      "outputs": [],
      "source": [
        "# If running locally you can add parameter visualize='True' if you want to see the training process (not on Colab).\n",
        "# Note that this will slow the training process\n",
        "environment = Environment.create(environment='gym', level='SpaceInvaders-v0')\n",
        "\n",
        "episode_reward = []\n",
        "episodeTimes = []\n",
        "episodeTimeSteps = []\n",
        "trainingStart = time.time()\n",
        "\n",
        "# Train for 100 episodes\n",
        "for i in range(100):\n",
        "\n",
        "    # Initialize episode\n",
        "    states = environment.reset()\n",
        "    terminal = False\n",
        "    rewardTotal = 0\n",
        "    currentEpisodeTimeSteps = 0\n",
        "    episodeStart = time.time()\n",
        "\n",
        "    # Main training loop\n",
        "    while not terminal:\n",
        "        # Episode timestep\n",
        "        currentEpisodeTimeSteps += 1\n",
        "        actions = agent.act(states=states)\n",
        "        states, terminal, reward = environment.execute(actions=actions)\n",
        "        agent.observe(terminal=terminal, reward=reward)\n",
        "        rewardTotal += reward\n",
        "    \n",
        "    # End of episode\n",
        "    episodeEnd = time.time()\n",
        "    timeEpisode = episodeEnd - episodeStart\n",
        "    episodeTimes.append(timeEpisode)\n",
        "    episode_reward.append(rewardTotal)\n",
        "    episodeTimeSteps.append(currentEpisodeTimeSteps)  \n",
        "    print('End of episode', i)\n",
        "    # Save info about episodes every 10 episodes\n",
        "    if len(episodeTimes) == 10:\n",
        "        with open(root_path +'Atari/Episodes/AGENT_NAME/rewards_per_episode.txt', 'a') as f:\n",
        "            for item in episode_reward:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        \n",
        "        with open(root_path +'Atari/Episodes/AGENT_NAME/timesteps_per_episode.txt', 'a') as f:\n",
        "            for item in episodeTimeSteps:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        \n",
        "        with open(root_path +'Atari/Episodes/AGENT_NAME/times_per_episode.txt', 'a') as f:\n",
        "            for item in episodeTimes:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        episode_reward = []\n",
        "        episodeTimes = []\n",
        "        episodeTimeSteps = []\n",
        "    # Save agent every 10 episodes.\n",
        "    # This can potentially be done with the \"saver\" setting in the agent too.\n",
        "    if i % 10 == 9:\n",
        "      agent.save(directory=root_path + 'Atari/AGENT_NAME')\n",
        "    \n",
        "trainingEnd = time.time()\n",
        "trainingTime = trainingEnd - trainingStart\n",
        "\n",
        "agent.close()\n",
        "environment.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check results of training:\n",
        "\n",
        "You can run this cell to load your agent and check some metrics about the current performance. Note how the training loop is slightly different, not having agent.observe() and agent.act() having the parameters independent and deterministic. That allows us to check the performance of the agent without training it."
      ],
      "metadata": {
        "id": "kmOJxSwe3axy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT4L2586137u"
      },
      "outputs": [],
      "source": [
        "agent = Agent.load(directory=root_path +'Atari/AGENT_NAME')\n",
        "environment = Environment.create(environment='gym', level='SpaceInvaders-v0')\n",
        "\n",
        "episodeTimes = []\n",
        "episodeTimeSteps = []\n",
        "episodeRewards = []\n",
        "for i in range(10):\n",
        "    episodeStart = time.time()\n",
        "    # Initialize episode\n",
        "    states = environment.reset()\n",
        "    terminal = False\n",
        "    currentEpisodeTimeSteps = 0\n",
        "    currentReward = 0\n",
        "    \n",
        "    while not terminal:\n",
        "        # Episode timestep\n",
        "        currentEpisodeTimeSteps += 1\n",
        "        actions = agent.act(states=states, independent = True, deterministic=True)\n",
        "        states, terminal, reward = environment.execute(actions=actions)\n",
        "        currentReward += reward\n",
        "    \n",
        "    episodeEnd = time.time()\n",
        "    timeEpisode = episodeEnd - episodeStart\n",
        "    episodeTimes.append(timeEpisode)\n",
        "    episodeTimeSteps.append(currentEpisodeTimeSteps)\n",
        "    episodeRewards.append(currentReward)\n",
        "    \n",
        "environment.close()\n",
        "    \n",
        "avgEpisodeTime = sum(episodeTimes) / len(episodeTimes)\n",
        "bestEpisodeTime = max(episodeTimes)\n",
        "avgEpisodeTimeSteps = sum(episodeTimeSteps) / len(episodeTimeSteps)\n",
        "bestEpisodeTimeSteps = max(episodeTimeSteps)\n",
        "avgEpisodeReward = sum(episodeRewards) / len(episodeRewards)\n",
        "bestEpisodeReward = max(episodeRewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzN4ryZWjzqk"
      },
      "outputs": [],
      "source": [
        "print(f\"Average time steps per episode: {avgEpisodeTimeSteps} timesteps\")\n",
        "print(f\"Episode with most timesteps: {bestEpisodeTimeSteps} timesteps\")\n",
        "print(f\"Average reward per episode: {avgEpisodeReward}\")\n",
        "print(f\"Best episode reward: {bestEpisodeReward}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "F7a8pjb62-Ls",
        "kc8EBNAX07Fz"
      ],
      "machine_shape": "hm",
      "name": "SpaceInvaders Vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}